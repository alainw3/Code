import torch.nn as nn
import torch 
import matplotlib.pyplot as plt 

GPT_CONFIG_124M ={
    "vocab_size" : 50257,           # vocabulary size
    "context_length" : 1024,        # context lenght
    "emb_dim" : 768,                # Embedding dimnsion
    "n_heads": 12,                  # Number of attention heads
    "n_layers":12,                  # Number of layers
    "drop_rate":0.1,                 # Dropout rate
    "qkv_bias" : False              # Query-Key-Value bias   

}



class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x,3))
        ))
    
class FeedFoward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"],4*cfg["emb_dim"]),         ##Expansion
            GELU(),                                             ## Activation
            nn.Linear(4* cfg["emb_dim"], cfg["emb_dim"]),       ## Contraction
        )

    def forward(self,x):
        return self.layers(x)



ffn = FeedFoward(GPT_CONFIG_124M)  
x = torch.rand(2,3,768)
out = ffn(x)
print(out.shape)